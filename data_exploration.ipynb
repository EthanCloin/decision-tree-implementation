{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e442766-3059-40bf-ae4d-992db9d67fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "PROJECT_01_ROOT = \".\"\n",
    "DATA_DIR = Path(PROJECT_01_ROOT).joinpath(\"CreditApproval\")\n",
    "\n",
    "training_data = pd.read_csv(DATA_DIR.joinpath(\"training.data\"), header=None)\n",
    "test_data = pd.read_csv(DATA_DIR.joinpath(\"test.data\"), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc898063-3661-43d2-9efb-e8125ec04709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f01c97-9754-40fa-9996-ab12e2b435b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"A\" + str(i) for i in range(1, 17)] # A1 - A16\n",
    "training_data.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af08dffc-5af6-4809-bd7a-d12c13c4ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull labels off dataset\n",
    "training_labels = training_data[\"A16\"]\n",
    "clean_data = training_data.drop(\"A16\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409058bd-1100-45a1-9df2-15c89140e7ac",
   "metadata": {},
   "source": [
    "`training_data` will not be mutated. I will make copies and manipulate those to determine median values.\n",
    "\n",
    "`clean_data` will be mutated to replace missing values with median values for appropriate columns. \n",
    "\n",
    "The below function handles updating attributes with their median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e5533a-e95a-497c-9161-5e270009466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_attribute_w_median(attribute, is_numeric):\n",
    "    \"\"\"fills missing w median value for attribute, returns updated column\"\"\"\n",
    "    # get a copy of dataset which lacks the missing values\n",
    "    training_copy = training_data.copy()\n",
    "    if not is_numeric:\n",
    "        # cut missing values from copied training set when finding median\n",
    "        training_copy = training_copy[training_copy[attribute] != \"?\"]\n",
    "        attribute_values = training_copy[attribute].sort_values()\n",
    "        attribute_median = attribute_values[len(attribute_values) // 2]\n",
    "        \n",
    "        # return col from original training set with replacement\n",
    "        return training_data[attribute].replace(\"?\", attribute_median)\n",
    "    else:\n",
    "        training_copy[attribute] = pd.to_numeric(training_copy[attribute], errors=\"coerce\")\n",
    "        attribute_median = training_copy[attribute].median()\n",
    "        training_copy[attribute] = training_copy[attribute].fillna(attribute_median)\n",
    "        return training_copy[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f71f91b-f729-4b24-a71d-fc8ee8b936b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1', 'A2', 'A4', 'A5', 'A6', 'A7', 'A14']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_missing_vals = training_data.columns[training_data.eq(\"?\").any(axis=0)].tolist()\n",
    "cols_with_missing_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5473ea-9378-47b4-bb3b-6d81b238130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below dict includes keys for each column with missing values\n",
    "# the key is attribute name and value represents whether it is_numeric\n",
    "missing_attributes = {\n",
    "    \"A1\": False,\n",
    "    \"A2\": True,\n",
    "    \"A4\": False,\n",
    "    \"A5\": False,\n",
    "    \"A6\": False,\n",
    "    \"A7\": False,\n",
    "    \"A14\": True,\n",
    "}\n",
    "\n",
    "# call my custom function to update all columns in the clean_data df\n",
    "for attribute, is_numeric in missing_attributes.items():\n",
    "    filled_column = update_attribute_w_median(attribute, is_numeric)\n",
    "    clean_data[attribute] = filled_column\n",
    "\n",
    "assert clean_data[\"A14\"].dtype == 'float64'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b01e6-4ed8-4e37-bdd4-a92d9d9aa6b6",
   "metadata": {},
   "source": [
    "At this stage, `clean_data` has been updated to replace all missing values with the appropriate median value. Now we can extract the datasets which will be relevant to our decision tree creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa5af21c-fecc-46ca-a1ef-815fbafc7823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   A1      550 non-null    object \n",
      " 1   A2      550 non-null    float64\n",
      " 2   A3      550 non-null    float64\n",
      " 3   A4      550 non-null    object \n",
      " 4   A5      550 non-null    object \n",
      " 5   A6      550 non-null    object \n",
      " 6   A7      550 non-null    object \n",
      " 7   A8      550 non-null    float64\n",
      " 8   A9      550 non-null    object \n",
      " 9   A10     550 non-null    object \n",
      " 10  A11     550 non-null    int64  \n",
      " 11  A12     550 non-null    object \n",
      " 12  A13     550 non-null    object \n",
      " 13  A14     550 non-null    float64\n",
      " 14  A15     550 non-null    int64  \n",
      "dtypes: float64(4), int64(2), object(9)\n",
      "memory usage: 64.6+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_data[\"A9\"] = clean_data[\"A9\"].astype(str)\n",
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ac3c45-0cfa-40fc-89a6-b994c6812755",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequentially_cross_validate(examples, labels, k=10):\n",
    "    fold_size = len(examples) // k\n",
    "    fold_msg = []\n",
    "    print(f\"cv {len(examples)} across {k} sets. folds are {fold_size} examples.\")\n",
    "    for i in range(k):\n",
    "        # determine sequential indices for validation set\n",
    "        v_start = i * fold_size\n",
    "        v_end = (i + 1) * fold_size if i != k-1 else len(examples)\n",
    "        \n",
    "        X_validation = examples.iloc[v_start:v_end]\n",
    "        y_validation = labels.iloc[v_start:v_end]\n",
    "\n",
    "        # grab remaining data for training set\n",
    "        X_train = pd.concat([examples.iloc[:v_start], examples.iloc[v_end:]])\n",
    "        y_train = pd.concat([labels.iloc[:v_start], labels.iloc[v_end:]])\n",
    "\n",
    "        print(f\"Fold {i}\\n\\tValidation from {v_start} to {v_end - 1} of len {len(X_validation)}\")\n",
    "        # train the model on training set\n",
    "        # run prediction on validation set\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb37d7d-3602-4c0e-9f85-13571b595c16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'greater_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.StrDType'>) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,objs\u001b[38;5;241m=\u001b[39m[clean_data, training_labels])\n\u001b[1;32m      5\u001b[0m tree\u001b[38;5;241m.\u001b[39mis_continuous(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA9\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# tree.print_tree()\u001b[39;00m\n",
      "File \u001b[0;32m~/handson-ml3/projects/01-project-decision-trees/decision_tree.py:121\u001b[0m, in \u001b[0;36mDecisionTreeNode.build_decision_tree\u001b[0;34m(self, dataset, parent_examples)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m DecisionTreeNode(\n\u001b[1;32m    115\u001b[0m         depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m DecisionTreeNode(\n\u001b[1;32m    118\u001b[0m         depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39mbuild_decision_tree(\n\u001b[0;32m--> 121\u001b[0m         dataset[\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m], dataset\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39mbuild_decision_tree(\n\u001b[1;32m    124\u001b[0m         dataset[dataset[attribute] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold], dataset\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# split on categorical attr case\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/homl3/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/homl3/lib/python3.10/site-packages/pandas/core/arraylike.py:52\u001b[0m, in \u001b[0;36mOpsMixin.__le__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__le__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__le__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/homl3/lib/python3.10/site-packages/pandas/core/series.py:5803\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5800\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   5801\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 5803\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/opt/conda/envs/homl3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/homl3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:131\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32mops.pyx:107\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'greater_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.StrDType'>) -> None"
     ]
    }
   ],
   "source": [
    "from decision_tree import DecisionTreeNode\n",
    "\n",
    "tree = DecisionTreeNode(max_depth=10, algorithm=\"C4.5\")\n",
    "dataset = pd.concat(axis=1,objs=[clean_data, training_labels])\n",
    "tree.is_continuous(\"A9\")\n",
    "tree.build_decision_tree(dataset)\n",
    "# tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ec355-136c-499a-9ae6-2abd17c057b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_split_points(dataset, attribute):\n",
    "    split_points = []\n",
    "\n",
    "    attr_values = np.sort(np.unique(dataset[attribute]))\n",
    "    # compute split points\n",
    "    print(len(attr_values))\n",
    "    for i, val in enumerate(attr_values):\n",
    "        # avoid outofbounds err\n",
    "        if i == len(attr_values) - 1:\n",
    "            print(\"skippp\")\n",
    "            continue\n",
    "        print(i)\n",
    "        midpoint = (val + attr_values[i + 1]) / 2\n",
    "        split_points.append(midpoint)\n",
    "    return split_points\n",
    "\n",
    "def copy_compute_split_points(dataset, attribute):\n",
    "    split_points = []\n",
    "\n",
    "    attr_values = np.sort(np.unique(dataset[attribute]))\n",
    "    # compute split points\n",
    "    for i, val in enumerate(attr_values):\n",
    "        # avoid outofbounds err\n",
    "        if i == len(attr_values) - 1:\n",
    "            continue\n",
    "        midpoint = (val + attr_values[i + 1]) / 2\n",
    "        split_points.append(midpoint)\n",
    "    return split_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3467e42-f23b-484a-88d7-e89db8968e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_compute_split_points(dataset, \"A2\")\n",
    "# x = np.sort(np.unique(dataset[\"A15\"]))\n",
    "# for i, val in enumerate(x):\n",
    "#     print(f\"{i}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c24662-3da2-4d25-9400-ff8c0d04df12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
