{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6291896-3119-4840-9d2b-a7813d04c399",
   "metadata": {},
   "source": [
    "# Stage 1: Data Access\n",
    "First we need to import the training and test data from the files in the same directory as our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e442766-3059-40bf-ae4d-992db9d67fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "PROJECT_01_ROOT = \".\"\n",
    "DATA_DIR = Path(PROJECT_01_ROOT).joinpath(\"CreditApproval\")\n",
    "\n",
    "training_data = pd.read_csv(DATA_DIR.joinpath(\"training.data\"), header=None)\n",
    "test_data = pd.read_csv(DATA_DIR.joinpath(\"test.data\"), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467624d-41a6-458e-897b-bd5cba61125e",
   "metadata": {},
   "source": [
    "# Stage 2: Data Preparation\n",
    "The data is provided without headers and includes some missing values.\n",
    "\n",
    "## Column Names\n",
    "Let's add the column names as outlined in the [dataset description](https://archive.ics.uci.edu/dataset/27/credit+approval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f01c97-9754-40fa-9996-ab12e2b435b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"A\" + str(i) for i in range(1, 17)] # A1 - A16\n",
    "training_data.columns = column_names\n",
    "test_data.columns = column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac39072-1bec-4049-a1b5-5555244a4b04",
   "metadata": {},
   "source": [
    "## Separate Labels and Examples\n",
    "\n",
    "The raw data as imported remains in the `training_data` and `test_data` variables.\n",
    "\n",
    "Let's use `clean_training` and `clean_test` to hold our prepared datasets, both of which will receive the same cleaning process to replace missing values with median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af08dffc-5af6-4809-bd7a-d12c13c4ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull labels off datasets\n",
    "training_labels = training_data[\"A16\"]\n",
    "test_labels = test_data[\"A16\"]\n",
    "\n",
    "clean_training = training_data.drop(\"A16\", axis=1)\n",
    "clean_test = test_data.drop(\"A16\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc0261-5883-48b1-be8f-8f7e272775d6",
   "metadata": {},
   "source": [
    "## Replace Missing with Median\n",
    "The below function uses the provided parameters to access a column from an 'original' dataframe, find the median, and replace any missing values with that median. \n",
    "\n",
    "Notice that we use the previously defined `training_data` dataframe to determine the median value, regardless of whether we call this function on an attribute in the test or training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e5533a-e95a-497c-9161-5e270009466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_attribute_w_median(attribute, is_numeric):\n",
    "    \"\"\"fills missing w median value for attribute, returns updated column\"\"\"\n",
    "    # get a copy of dataset which lacks the missing values\n",
    "    training_copy = training_data.copy()\n",
    "    if not is_numeric:\n",
    "        # cut missing values from copied training set when finding median\n",
    "        training_copy = training_copy[training_copy[attribute] != \"?\"]\n",
    "        attribute_values = training_copy[attribute].sort_values()\n",
    "        attribute_median = attribute_values[len(attribute_values) // 2]\n",
    "        \n",
    "        # return col from original training set with replacement\n",
    "        return training_data[attribute].replace(\"?\", attribute_median)\n",
    "    else:\n",
    "        training_copy[attribute] = pd.to_numeric(training_copy[attribute], errors=\"coerce\")\n",
    "        attribute_median = training_copy[attribute].median()\n",
    "        training_copy[attribute] = training_copy[attribute].fillna(attribute_median)\n",
    "        return training_copy[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f71f91b-f729-4b24-a71d-fc8ee8b936b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1', 'A2', 'A4', 'A5', 'A6', 'A7', 'A14']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_missing_vals = training_data.columns[training_data.eq(\"?\").any(axis=0)].tolist()\n",
    "cols_with_missing_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e5473ea-9378-47b4-bb3b-6d81b238130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below dict includes keys for each column with missing values\n",
    "# the key is attribute name and value represents whether it is_numeric\n",
    "missing_attributes = {\n",
    "    \"A1\": False,\n",
    "    \"A2\": True,\n",
    "    \"A4\": False,\n",
    "    \"A5\": False,\n",
    "    \"A6\": False,\n",
    "    \"A7\": False,\n",
    "    \"A14\": True,\n",
    "}\n",
    "\n",
    "# call my custom function to update all columns in the clean_training df\n",
    "for attribute, is_numeric in missing_attributes.items():\n",
    "    filled_column = update_attribute_w_median(attribute, is_numeric)\n",
    "    clean_training[attribute] = filled_column\n",
    "\n",
    "for attribute, is_numeric in missing_attributes.items():\n",
    "    filled_column = update_attribute_w_median(attribute, is_numeric)\n",
    "    clean_test[attribute] = filled_column\n",
    "\n",
    "assert clean_training[\"A14\"].dtype == 'float64'\n",
    "assert clean_test[\"A14\"].dtype == 'float64'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b01e6-4ed8-4e37-bdd4-a92d9d9aa6b6",
   "metadata": {},
   "source": [
    "# Stage 3: Training the Models\n",
    "\n",
    "The below method accepts a DecisionTreeNode and dataset, then performs a 10-fold sequential cross validation. The F1 scores are printed, and the best model + score are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ac3c45-0cfa-40fc-89a6-b994c6812755",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from decision_tree import DecisionTreeNode\n",
    "\n",
    "def sequentially_cross_validate(model, examples, labels, k=10):    \n",
    "    fold_size = len(examples) // k\n",
    "    # print(f\"cv {len(examples)} across {k} sets. folds are {fold_size} examples.\")\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for i in range(k):\n",
    "        # determine sequential indices for validation set\n",
    "        v_start = i * fold_size\n",
    "        v_end = (i + 1) * fold_size if i != k-1 else len(examples)\n",
    "        \n",
    "        X_validation = examples.iloc[v_start:v_end]\n",
    "        y_validation = labels.iloc[v_start:v_end]\n",
    "\n",
    "        # grab remaining data for training set\n",
    "        X_train = pd.concat([examples.iloc[:v_start], examples.iloc[v_end:]])\n",
    "        y_train = pd.concat([labels.iloc[:v_start], labels.iloc[v_end:]])\n",
    "        \n",
    "        # print(f\"Fold {i}\\n\\tValidation from {v_start} to {v_end - 1} of len {len(X_validation)}\")\n",
    "        # train the model on training set\n",
    "        training_set = pd.concat(axis=1,objs=[X_train, y_train])\n",
    "        model.build_decision_tree(training_set)\n",
    "        \n",
    "        # run prediction on validation set\n",
    "        validation_set = pd.concat(axis=1,objs=[X_validation, y_validation])\n",
    "        predicted_labels = model.predict(validation_set)\n",
    "\n",
    "        # compute f1 score and store the best one\n",
    "        f1 = f1_score(y_validation, predicted_labels, labels=['+', '-'], pos_label='+')\n",
    "        print(f\"Fold {i} model has F1 Score: {f1}\")\n",
    "        if f1 > best_score:\n",
    "            best_score = f1\n",
    "            best_model = model\n",
    "    return best_model, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e53355-90ba-49f7-bf6d-88f28cc266f9",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "We now create an instance of the DecisionTreeNode class for each of the algorithms, running them through the custom cross validation method. We can observe the scores for each of the folds from the printed output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb37d7d-3602-4c0e-9f85-13571b595c16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 model has F1 Score: 0.7692307692307692\n",
      "Fold 1 model has F1 Score: 0.7659574468085107\n",
      "Fold 2 model has F1 Score: 0.7916666666666667\n",
      "Fold 3 model has F1 Score: 0.7659574468085107\n",
      "Fold 4 model has F1 Score: 0.7272727272727272\n",
      "Fold 5 model has F1 Score: 0.8571428571428572\n",
      "Fold 6 model has F1 Score: 0.823529411764706\n",
      "Fold 7 model has F1 Score: 0.8076923076923077\n",
      "Fold 8 model has F1 Score: 0.847457627118644\n",
      "Fold 9 model has F1 Score: 0.7346938775510203\n"
     ]
    }
   ],
   "source": [
    "c45_tree = DecisionTreeNode(algorithm=\"C4.5\")\n",
    "best_c45, c45_score = sequentially_cross_validate(c45_tree, clean_training, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312c678f-b1a6-4c84-bb69-ac7cc6b0b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 model has F1 Score: 0.7317073170731708\n",
      "Fold 1 model has F1 Score: 0.72\n",
      "Fold 2 model has F1 Score: 0.7727272727272727\n",
      "Fold 3 model has F1 Score: 0.7659574468085107\n",
      "Fold 4 model has F1 Score: 0.7441860465116279\n",
      "Fold 5 model has F1 Score: 0.84\n",
      "Fold 6 model has F1 Score: 0.6938775510204083\n",
      "Fold 7 model has F1 Score: 0.8260869565217391\n",
      "Fold 8 model has F1 Score: 0.7200000000000001\n",
      "Fold 9 model has F1 Score: 0.6909090909090909\n"
     ]
    }
   ],
   "source": [
    "cart_tree = DecisionTreeNode(algorithm=\"CART\")\n",
    "best_cart, cart_score = sequentially_cross_validate(cart_tree, clean_training, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562bd14-3110-4a12-9fd3-9c63280da901",
   "metadata": {},
   "source": [
    "# Stage 4: Predicting on the Test Set\n",
    "Now that we have trained our models and selected the best performers from each category, we can use them to predict labels against our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22393f32-6ee4-45fe-ba62-de9063990bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7538461538461538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_c45_predictions = best_c45.predict(clean_test)\n",
    "c45_f1 = f1_score(test_labels, best_c45_predictions, labels=['+', '-'], pos_label='+')\n",
    "c45_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4c24b1-1fc4-4ed7-964d-6081c5b0a03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931034482758621"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cart_predictions = best_cart.predict(clean_test)\n",
    "cart_f1 = f1_score(test_labels, best_cart_predictions, labels=['+', '-'], pos_label='+')\n",
    "cart_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc0ecd-8aa6-4cb4-8d88-daab7ccfa26a",
   "metadata": {},
   "source": [
    "# Discussing Results\n",
    "\n",
    "| Model | Dataset | Score |\n",
    "| ----- | ------- | ----- |\n",
    "| C4.5 | Training | 0.8571428571428572 |\n",
    "| C4.5 | Test | 0.7538461538461538 |\n",
    "| CART | Training | 0.84 |\n",
    "| CART | Test | 0.7931034482758621 |\n",
    "\n",
    "\n",
    "The CART algorithm had a noticably higher F1 score on the test set. This is somewhat counter-intuitive, since looking at the scores for the training data, the C4.5 algorithm had consistently better predictions. \n",
    "\n",
    "This suggests that C4.5 may be more prone to overfitting than the CART algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
